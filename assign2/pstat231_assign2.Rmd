---
title: "Solutions to HW 2"
author: "Shaoyi Zhang"
date: "April 15th, 2016"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',warning=FALSE)
```
```{r}
library(ISLR)
library(ggplot2)
library(data.table)
```

```{r}
summary(iris)

RawData = iris
RawData$Species = as.numeric(RawData$Species)
str(RawData)
apply(RawData,2,var)

head(RawData) #  See first elements of iris data set
predictorX = subset(RawData,select = -Species)
responseY = subset(RawData,select = Species)

pr.out = prcomp(predictorX, center = T, scale=T)
summary(pr.out)
pr.out$rotation

#names(pr.out)
plot(pr.out,type="l")
biplot(pr.out,scale = 0)

#cumsum(pve)
```

## Question 1.2

Dimension means which variable contribute most to the variance
At second PC, the loding of Sepal length is the greatest and the loading of Petal width and length is near zero. Thus, we can say PC2 is the dimension of Sepal.


## Question 1.c
```{r,}
require(devtools)
install_github("vqv/ggbiplot")

library(ggbiplot)
RawData = iris
RawData$Species = as.numeric(RawData$Species)

iris.pca <- prcomp(subset(RawData,select = -Species), scale. = TRUE)
ggbiplot(iris.pca, obs.scale = 1, var.scale = 1,
  groups = iris$Species, ellipse = TRUE, circle = TRUE) +
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal', legend.position = 'top')
```

## Question 1.d
```{r,warning=F}
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
pve
```
##### The PVE of the first pricipal component is 
```{r,echo=F}
pve[1]
```

##### The PVE of the second pricipal component is
```{r,echo=F}
pve[2]
```

##### Plot for PVE:
```{r,echo=FALSE}
plot(pve, xlab = " Principal Component ", ylab = " Proportion of Variance Explained ", ylim = c(0,1),type = 'b')

```


## Question 2.a
```{r}
require(class)
require(boot)
df = data.frame(iris)
df[,c(1,2,3,4)]
X = df[,1:4]
p.YTrain = NULL
train.error.rate = NULL
for(i in 1:50){
  set.seed(1)
  #XTrain = sample(x=iris[,1:4])
  #p.YTest = knn(XTrain,X-XTrain,X[XTrain],k=i)
  p.YTrain = knn.cv(train = X, cl = df$Species, k = i)
  train.error.rate[i] = mean(df$Species != p.YTrain)
}
which.min(train.error.rate)
min(train.error.rate)
require(data.table)
Error.rates<-data.table("k"=1:50,"Train.error.rate"=test.error.rate)

gg4<-ggplot(Error.rates)+geom_line(aes(x=k,y=Train.error.rate), color="Red")+xlab("k")+ylab("Error rates")+ggtitle("Train Error Rate (Red)")
gg4
train.error.rate

myirisnumeric = iris[,1:4]
myirisclass = iris[,5]
str(myirisnumeric)
str(myirisclass)
myclassnumeric = as.numeric(myirisclass)
str(myclassnumeric)
myclassfactor = as.factor(myclassnumeric)
knn.cv(iris[,1:4],cl=iris$Species,k=10)
```
## Question 2.b
```{r}
require(MASS)
  
irisdf = data.frame(iris)
train.error.rate = NULL
lda.fit = lda(Species~.,data=irisdf,CV=T)
names(lda.fit)
lda.LOOCV<-data.table("LOOCV class"=lda.fit$class,"Post prob"=lda.fit$posterior)
for (i in 1:dim(irisdf)[1]){
  train.error.rate[i] = mean(irisdf$Species != lda.LOOCV$`LOOCV class`)
}

lda.fit$class
gglda = ggplot(lda.LOOCV)+geom_line(aes(x=))

str(lda.LOOCV)
which.min(train.error.rate)
min(train.error.rate)
require(data.table)
Error.rates<-data.table("k"=1:50,"Train.error.rate"=test.error.rate)

gg4<-ggplot(Error.rates)+geom_line(aes(x=k,y=Train.error.rate), color="Red")+xlab("k")+ylab("Error rates")+ggtitle("Train Error Rate (Red)")
gg4
train.error.rate

lda.pred = predict(lda.fit, irisdf)
summary(lda.pred)


Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
                   Sp = rep(c("s","c","v"), rep(50,3)))
str(Iris)
train <- sample(1:150, 75)
table(Iris$Sp[train])
## your answer may differ
##  c  s  v
## 22 23 30
z <- lda(Sp ~ ., Iris, prior = c(1,1,1)/3, subset = train)
predict(z, Iris[-train, ])$class
##  [1] s s s s s s s s s s s s s s s s s s s s s s s s s s s c c c
## [31] c c c c c c c v c c c c v c c c c c c c c c c c c v v v v v
## [61] v v v v v v v v v v v v v v v
(z1 <- update(z, . ~ . - Petal.W.))  

  
```
## Additional Exercise PSTAT 231
### Question 1

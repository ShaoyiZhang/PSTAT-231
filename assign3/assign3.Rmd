---
title: "Solutions to HW3"
author: "Shaoyi Zhang"
date: "April 30th, 2016"
output: pdf_document
---

## Question 1

```{r global_options, include=FALSE,warning=F}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',warning=FALSE)
```

```{r}
# set up data frame
setwd("/Users/Shawn/Desktop/PSTAT 231/PSTAT-231/assign3")
spam = read.table("spambase.dat",header=T,sep="")
#summary(spam)
spam$y = factor(spam$y,levels=c(0,1),labels=c("good","spam"))

# partition the data set
# train set size = sample size - 1000
# test set size = 1000
train_size <- floor(nrow(spam)-1000)

# set the seed to make your partition reproductible
set.seed(1)
train_index <- sample(seq_len(nrow(spam)), size = train_size)

train <- spam[train_index, ]
test <- spam[-train_index, ]

```

Now, we can start build the decision tree

```{r}
require(tree)
spam.tree = tree(y~.,data=train)

cv.tree(spam.tree,FUN=prune.misclass)
```

The optimal tree size is 11

```{r}
prune.spam.tree = prune.misclass(spam.tree,best = 11)
```

```{r, echo=FALSE}
# plotting unpruned tree
plot(spam.tree,main="")
title(main=list("unpruned decision tree without option",cex=2,font=3))
text(spam.tree,pretty = 0)

# plot pruned tree
plot(prune.spam.tree,main="pruned decision tree without option")
title(main=list("pruned decision tree without option",cex=2,font=3))
text(prune.spam.tree,pretty = 0)
```

```{r}
# make prediction on test set(unpruned)
spam.tree.pred = predict(spam.tree,test,type="class")
conti.table = table(spam.tree.pred,test$y)

# make prediction on test set(pruned)
prune.pred = predict(prune.spam.tree,test,type="class")
prune.conti.table = table(prune.pred,test$y)

# construct error rate vector
test.error.rates = vector()
model.index = 1

# compute the test error rate
test.error.rates[model.index] = (prune.conti.table[3] + prune.conti.table[2])/nrow(test)
model.index = model.index + 1
test.error.rates
```

Then, let's try a decision tree with options

```{r}
# decision tree with option
spam.tree.option = tree(y~.,data=train,control=tree.control(nrow(spam),mincut=2,minsize=5,mindev=0.001))
cv.tree(spam.tree.option,FUN=prune.misclass)

# the optimal tree size is 91
prune.spam.option = prune.misclass(spam.tree.option,best = 91)


spam.option.pred = predict(prune.spam.option,test,type="class")
conti.table = table(spam.option.pred,test$y)

test.error.rates[model.index] = (conti.table[3] + conti.table[2])/nrow(test)
model.index = model.index + 1
test.error.rates
```

Tree bagging model
```{r}
#install.packages("randomForest")
library(randomForest)

set.seed(1)
bag.spam = randomForest(y~.,data=train,mtry=(ncol(train)-1),importance=T)
bag.spam

bag.pred = predict(bag.spam,test,type="class")
bag.conti.table = table(bag.pred,test$y)

test.error.rates[model.index] = (bag.conti.table[3] + bag.conti.table[2])/nrow(test)
model.index = model.index + 1
test.error.rates
```

Random Forest model
```{r}
set.seed(1)
rand.spam = randomForest(y~.,data=train,mtry=floor(sqrt(ncol(test)-1)),importance=T)
rand.spam

rand.pred = predict(rand.spam,test,type="class")
rand.conti.table = table(rand.pred,test$y)

test.error.rates[model.index] = (conti.table[3] + conti.table[2])/nrow(test)
model.index = model.index + 1
test.error.rates
```

k-NN classification
```{r}
require(class)
require(boot)
require(ggplot2)
p.YTrain = NULL
train.error.rate = NULL
for(i in 1:100){
  set.seed(3)
  # use test to determine optimal knn ??
  p.YTrain = knn.cv(train = test[,1:(ncol(test)-1)], cl = test$y, k = i)
  train.error.rate[i] = mean(train$y != p.YTrain)
}

gg4<-ggplot(data.frame(x = 1:100,y = train.error.rate))+geom_line(aes(x=x,y=y), color="Red")+xlab("k")+ylab("Error rates")+ggtitle("Train Error Rate (Red)")+geom_vline(xintercept = which.min(train.error.rate),lty = "dashed")+geom_hline(yintercept = train.error.rate[which.min(train.error.rate)],lty="dashed")
gg4
 
```

## Part 3
```{r}
require(ROCR)
require(data.table)
bag.pred = data.table(predict(bag.spam,test,type="prob")[,-1])
pred.bag = prediction(bag.pred,test$y)
perf.bag = performance(pred.bag,measure="tpr",x.measure="fpr")
plot(perf.bag,col="blue",lwd=3)

rf.pred = data.table(predict(rand.spam,test,type="prob"))
#rf.pred
pred.rf = prediction(rf.pred[,c(spam)],test$y)
perf.rf = performance(pred.rf,measure = "tpr",x.measure = "fpr")
plot(perf.rf,lwd=3,add=T,col="green")

knn.pred = knn(train[,-ncol(train)],test[,-ncol(test)],train$y,k=4,prob=T)
knn.p=1-attributes(knn.pred)$prob
knn.p[knn.p==0]=0
#knn.p
pred.knn = prediction(knn.p,test$y)
#pred.knn
perf.knn = performance(pred.knn,measure = "tpr",x.measure = "fpr")
plot(perf.knn,lwd=3,add=T,col="red")
legend(0.6,0.6,c('Bagging tree','random forest','k-NN'),col=c('blue','green','red'),lwd=3)
```

## Question 2
```{r}
require(ggplot2)
funcs = ggplot(data.frame(x=c(0,1)),aes(x))
GiniIndex = function(p){2*p*(1-p)}
class.error = function(p) {min(p,(1-p))}
# class error rate measuere is wierd

cross.entropy = function(p) {-p*log(p)-(1-p)*log(1-p)}
funcs + stat_function(fun=GiniIndex) + stat_function(fun=class.error) + stat_function(fun=cross.entropy) 
```


## Additional Questions for PSTAT 231
## Question 3
##### Majority vote approach:

Among the 10 estimates, we have 4 prediction that has a probability less than 0.5. Since we have more estimates indicating "class is red", we conclude that the class is red.

##### Average probabiltiy approach:

The average probability among the 10 estimates is:
```{r}
(0.1+0.15+0.2+0.2+0.55+0.6+0.6+0.65+0.7+0.75)/10
```
Since the average probability is below 0.5, we conclude that the class is NOT red.